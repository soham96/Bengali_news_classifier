{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bengali News Classifier using Pretrained Embeddings\n",
    "\n",
    "In this notebook, we will build a TensorFlow model to predict news classes based on their headline using embeddings that we trained with news articles.\n",
    "\n",
    "First we import all the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=[]\n",
    "\n",
    "symbols = {0: 'PAD',1: 'UNK'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filepath, vocab, dim):\n",
    "\n",
    "    word_vocab =[]\n",
    "    embedding_matrix = []\n",
    "    word_vocab.extend(['PAD','UNK'])\n",
    "    embedding_matrix.append(np.random.uniform(-1.0, 1.0, (1,dim))[0])\n",
    "    embedding_matrix.append(np.random.uniform(-1.0, 1.0, (1,dim))[0])\n",
    "\n",
    "    \n",
    "    with open(filepath, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i==0:\n",
    "                continue\n",
    "            if line.split()[0] in vocab:\n",
    "                word_vocab.append(line.split()[0].strip())\n",
    "                embedding_matrix.append(np.asarray(line.split()[1:], dtype='float32'))\n",
    "                \n",
    "        \n",
    "    return {'word_vocab': word_vocab,'embedding_matrix': np.reshape(embedding_matrix,[-1,dim]).astype(np.float32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(filenames):\n",
    "    \n",
    "    extracted_title=[]\n",
    "    extracted_cls=[]\n",
    "    \n",
    "    for filename in filenames:\n",
    "        with open(os.path.join('data', filename), 'r') as f:\n",
    "            articles=json.load(f)\n",
    "\n",
    "        for article in articles['articles']:\n",
    "            extracted_title.append(article['title'].strip())\n",
    "            extracted_cls.append(article['label'].strip())\n",
    "    \n",
    "    return extracted_title, extracted_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    cls=[]\n",
    "    text=[]\n",
    "\n",
    "    with open(os.path.join('data', filename+'.txt'), 'r') as f:\n",
    "        for line in f:\n",
    "            cls.append(line.split('||')[0])\n",
    "            text.append(line.split('||')[1])\n",
    "    \n",
    "    return cls, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(sentences):\n",
    "    new_sentences=[]\n",
    "    exclude = list(set(string.punctuation))\n",
    "    exclude.extend([\"’\", \"‘\", \"—\"])\n",
    "    for sentence in sentences:\n",
    "        s = ''.join(ch for ch in sentence if ch not in exclude)\n",
    "        new_sentences.append(s)\n",
    "    \n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_strings(texts, replace):\n",
    "    new_texts=[]\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    english_pattern=re.compile('[a-zA-Z0-9]+', flags=re.I)\n",
    "    \n",
    "    for text in tqdm(texts):\n",
    "        for r in replace:\n",
    "            text=text.replace(r[0], r[1])\n",
    "        text=emoji_pattern.sub(r'', text)\n",
    "        text=english_pattern.sub(r'', text)\n",
    "        text=re.sub(r'\\s+', ' ', text).strip()\n",
    "        new_texts.append(text)\n",
    "\n",
    "    return new_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(df):\n",
    "    cols=df.cls.drop_duplicates().values\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import sklearn\n",
    "\n",
    "    train=pd.DataFrame()\n",
    "    test=pd.DataFrame()\n",
    "\n",
    "    for col in cols:\n",
    "        split_df=df[df.cls == col]\n",
    "        train_df, test_df= train_test_split(split_df, test_size=0.2)\n",
    "        train=[train, train_df]\n",
    "        test=[test, test_df]\n",
    "        train=pd.concat(train)\n",
    "        test=pd.concat(test)\n",
    "    \n",
    "    return sklearn.utils.shuffle(train), sklearn.utils.shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_common(sentences, unique_tokens, top):\n",
    "    new_sentences=[]\n",
    "    common=Counter(unique_tokens).most_common(top)\n",
    "    common=list(list(zip(*common))[0])\n",
    "\n",
    "    for sentence in tqdm(sentences):\n",
    "        sentence=sentence.split()\n",
    "        words=[word for word in sentence if word not in common]\n",
    "        new_sentences.append(' '.join(words).strip())\n",
    "    \n",
    "    return new_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['বিশ্বসাথে যোগে যেথায় বিহারো', 'কফি হাউসে ধূমপান নিয়ে বচসায় গ্রাহককে মারধরের অভিযোগ কর্মীর বিরুদ্ধে', 'ফের বদলি আমলা, এবার কোপে সুন্দরবন উন্নয়ন দফতরের সচিব', 'বৈশাখী নন, অভিষেকের কারণেই মন্ত্রিত্ব ছেড়েছেন শোভন: মুকুল', 'বড়দিনে উধাও শীত, পারদ ঊর্ধ্বমুখী', 'স্বাদে স্বাধীনতা', 'অথ সিন্ডিকেট কথা...', 'জিসটিতে জোর ধাক্কা বাংলার মিষ্টি শিল্পে', 'অসমাপ্ত গল্প', 'দেশকে বাঁচানোর দায়িত্ব নিতে হবে ছাত্রসমাজকেই: মমতা বন্দ্যোপাধ্যায়']\n",
      "['kolkata', 'kolkata', 'kolkata', 'kolkata', 'kolkata', 'kolkata', 'kolkata', 'kolkata', 'kolkata', 'kolkata']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14205/14205 [00:00<00:00, 77585.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['বিশ্বসাথে যোগে যেথায় বিহারো', 'কফি হাউসে ধূমপান নিয়ে বচসায় গ্রাহককে মারধরের অভিযোগ কর্মীর বিরুদ্ধে', 'ফের বদলি আমলা এবার কোপে সুন্দরবন উন্নয়ন দফতরের সচিব', 'বৈশাখী নন অভিষেকের কারণেই মন্ত্রিত্ব ছেড়েছেন শোভন মুকুল', 'বড়দিনে উধাও শীত পারদ ঊর্ধ্বমুখী', 'স্বাদে স্বাধীনতা', 'অথ সিন্ডিকেট কথা', 'জিসটিতে জোর ধাক্কা বাংলার মিষ্টি শিল্পে', 'অসমাপ্ত গল্প', 'দেশকে বাঁচানোর দায়িত্ব নিতে হবে ছাত্রসমাজকেই মমতা বন্দ্যোপাধ্যায়']\n",
      "55794\n",
      "Counter({'state': 14274, 'sports': 12780, 'kolkata': 10133, 'entertainment': 7737, 'national': 7569})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52493/52493 [00:00<00:00, 161397.67it/s]\n"
     ]
    }
   ],
   "source": [
    "titles, cls=extract_text(['zeenews_articles.txt', 'anandabazar_articles.txt', 'ebala_articles.txt'])\n",
    "print(titles[:10])\n",
    "print(cls[:10])\n",
    "replace=[('\\u200c', ' '),\n",
    "         ('\\u200d', ' '),\n",
    "        ('\\xa0', ' '),\n",
    "        ('\\n', ' '),\n",
    "        ('\\r', ' ')]\n",
    "\n",
    "titles=remove_punc(titles)\n",
    "titles=replace_strings(titles, replace)\n",
    "print(titles[:10])\n",
    "\n",
    "\n",
    "labels, text=read_data('classification')\n",
    "\n",
    "yo=read_data('anandabazar_classification')\n",
    "labels.extend(yo[0])\n",
    "text.extend(yo[1])\n",
    "\n",
    "yo=read_data('ebala_classification')\n",
    "labels.extend(yo[0])\n",
    "text.extend(yo[1])\n",
    "\n",
    "text=remove_punc(text)\n",
    "\n",
    "cls.extend(labels)\n",
    "titles.extend(text)\n",
    "\n",
    "yo=list(set(zip(cls, titles)))\n",
    "print(len(yo))\n",
    "\n",
    "# Removing Travel and World class as they are low in number\n",
    "df=pd.DataFrame(yo, columns=['cls', 'titles'])\n",
    "df=df.replace(['international', 'sport', 'nation'], ['world', 'sports', 'national'])\n",
    "df=df[df.cls!='travel']\n",
    "df=df[df.cls!='world']\n",
    "\n",
    "vocab=[]\n",
    "\n",
    "titles=df.titles.values\n",
    "cls=df.cls.values\n",
    "\n",
    "print(Counter(df.cls.values))\n",
    "\n",
    "for sentence in titles:\n",
    "    for word in sentence.split():\n",
    "        vocab.append(word.lower())\n",
    "\n",
    "df['titles']=remove_common(titles, vocab, 20)\n",
    "\n",
    "df=df[df['titles'].str.len()>2]\n",
    "df=df.drop_duplicates()\n",
    "\n",
    "vocab = list(set(vocab))\n",
    "common=Counter(vocab).most_common(50000)\n",
    "vocab=list(list(zip(*common))[0])\n",
    "\n",
    "embeddings=load_embeddings('expanded_news_vec.txt', vocab, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36005\n",
      "36005\n"
     ]
    }
   ],
   "source": [
    "load_embedding_matrix = embeddings['embedding_matrix']\n",
    "shape_word_vocab = embeddings['word_vocab']\n",
    "\n",
    "int_to_vocab = {}\n",
    "\n",
    "for index_no,word in enumerate(shape_word_vocab):\n",
    "    int_to_vocab[index_no] = word\n",
    "int_to_vocab.update(symbols)\n",
    "\n",
    "\n",
    "vocab_to_int = {word:index_no for index_no , word in int_to_vocab.items()}\n",
    "\n",
    "print(len(shape_word_vocab))\n",
    "\n",
    "print(len(load_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(titles):\n",
    "    encoded_data = []\n",
    "\n",
    "    for sentence in titles:\n",
    "        sentence_ =[]\n",
    "        for word in sentence.split():\n",
    "            if word.lower() in vocab_to_int:\n",
    "                sentence_.append(vocab_to_int[word.lower()])\n",
    "            else:\n",
    "                sentence_.append(vocab_to_int['UNK'])\n",
    "        encoded_data.append(sentence_)\n",
    "\n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['state' 'state' 'sports' 'sports' 'state' 'entertainment' 'national'\n",
      " 'state' 'state' 'sports']\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df=split(df)\n",
    "\n",
    "train_label=pd.Series(train_df.cls).str.get_dummies().values\n",
    "test_label=pd.Series(test_df.cls).str.get_dummies().values\n",
    "\n",
    "train_encoded_data=encode_data(train_df.titles.values)\n",
    "test_encoded_data=encode_data(test_df.titles.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, train):\n",
    "\n",
    "        sentence  = tf.placeholder(name='input_sentence',shape=[None,None],dtype=tf.int32)\n",
    "        cls_= tf.placeholder(name='cls',shape=[None, 5],dtype=tf.int32)\n",
    "        y_true_cls = tf.argmax(cls_, axis=1)\n",
    "\n",
    "        self.placeholders = {'sentence':sentence,'cls':cls_}\n",
    "\n",
    "        Word_embedding = tf.get_variable(name=\"Word_embedding\", \n",
    "                                     shape=[len(vocab),200], \n",
    "                                     initializer=tf.constant_initializer(np.array(load_embedding_matrix)), \n",
    "                                     trainable=train\n",
    "                                    )\n",
    "\n",
    "\n",
    "        embedding_lookup= tf.nn.embedding_lookup(Word_embedding,sentence)\n",
    "\n",
    "        sequence_leng = tf.count_nonzero(sentence,axis=1)\n",
    "\n",
    "        with tf.variable_scope('forward'):\n",
    "            fr_cell = tf.contrib.rnn.LSTMCell(num_units=128)\n",
    "#             dropout_fr = tf.contrib.rnn.DropoutWrapper(fr_cell)\n",
    "\n",
    "        with tf.variable_scope('backward'):\n",
    "            bw_cell = tf.contrib.rnn.LSTMCell(num_units=128)\n",
    "#             dropout_bw = tf.contrib.rnn.DropoutWrapper(bw_cell)\n",
    "\n",
    "        with tf.variable_scope('encoder') as scope:\n",
    "            model,last_state = tf.nn.bidirectional_dynamic_rnn(fr_cell, bw_cell, inputs=embedding_lookup,\n",
    "                                                               sequence_length=np.zeros(32)+15,\n",
    "                                                               dtype=tf.float32)\n",
    "\n",
    "        concat_output = tf.concat([last_state[0].c,last_state[1].c],axis=-1)\n",
    "\n",
    "        net = tf.layers.dense(inputs=concat_output, name='layer_fc1',\n",
    "                      units=1024, activation=tf.nn.relu)\n",
    "        net = tf.layers.dense(inputs=net, name='layer_fc2',\n",
    "                      units=512, activation=tf.nn.relu)\n",
    "        net = tf.layers.dense(inputs=net, name='layer_fc3',\n",
    "                      units=128, activation=tf.nn.relu)\n",
    "        net = tf.layers.dense(inputs=net, name='layer_fc_out',\n",
    "                      units=5, activation=None)\n",
    "        \n",
    "        logits = net\n",
    "\n",
    "        #prediction\n",
    "        probability = tf.nn.softmax(logits)\n",
    "        prediction  = tf.argmax(probability, axis=1)\n",
    "\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=cls_)\n",
    "\n",
    "        cost = tf.reduce_mean(cross_entropy)\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "        #accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction, y_true_cls), tf.float32))\n",
    "#         f1=tf.contrib.metrics.f1_score(y_true_cls, prediction)\n",
    "\n",
    "        self.acc={'accuracy':accuracy, 'cls':y_true_cls, 'cost':cost}\n",
    "        self.output = {'loss':cost,'accuracy':accuracy,'logits': logits,'check1':embedding_lookup,'check2':model}\n",
    "        self.train = optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    \n",
    "    for item in batch:\n",
    "        if len(item[0])>0:\n",
    "            x.append(item[0])\n",
    "            y.append(item[1])\n",
    "    from keras.preprocessing import sequence\n",
    "    x=sequence.pad_sequences(x, maxlen=15)\n",
    "            \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch: 1, Cost: 1.0021956870770892, Acc: 0.5933010774142059\n",
      "Testing:  Epoch: 1, Cost: 0.922282394700157, Acc: 0.6320886581469649\n",
      "Training: Epoch: 2, Cost: 0.8590483333430857, Acc: 0.6587190742218675\n",
      "Testing:  Epoch: 2, Cost: 0.8717126891064567, Acc: 0.6588458466453674\n",
      "Training: Epoch: 3, Cost: 0.7743864111774746, Acc: 0.6957551875498803\n",
      "Testing:  Epoch: 3, Cost: 0.8767227778038659, Acc: 0.6613418530351438\n",
      "Training: Epoch: 4, Cost: 0.694750135147562, Acc: 0.7277783320031923\n",
      "Testing:  Epoch: 4, Cost: 0.8941127435087015, Acc: 0.6612420127795527\n",
      "Training: Epoch: 5, Cost: 0.6098273073732615, Acc: 0.7636671987230647\n",
      "Testing:  Epoch: 5, Cost: 0.9727417603849222, Acc: 0.652555910543131\n"
     ]
    }
   ],
   "source": [
    "training_data=list(zip(train_encoded_data, train_label))\n",
    "\n",
    "tot=int(len(training_data)/32)\n",
    "batched_train=[training_data[32*i : 32*(i+1)] for i in range(tot)]\n",
    "\n",
    "\n",
    "testing_data=list(zip(test_encoded_data, test_label))\n",
    "\n",
    "tot=int(len(testing_data)/32)\n",
    "batched_test=[testing_data[32*i : 32*(i+1)] for i in range(tot)]\n",
    "\n",
    "\n",
    "accuracy=0\n",
    "cost=0\n",
    "f1=0\n",
    "train_accuracy=0\n",
    "train_cost=0\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model=LSTM(train=False)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(5):\n",
    "        for batch in batched_train:\n",
    "            x, y=get_batch(batch)\n",
    "            \n",
    "            if len(x)!=32:\n",
    "                print(len(x))\n",
    "                continue\n",
    "\n",
    "            sess.run(model.train,feed_dict={model.placeholders['sentence']: np.reshape(x,[-1,15]), model.placeholders['cls']:np.reshape(y,[-1,5])})\n",
    "            train_acc=sess.run(model.acc,feed_dict={model.placeholders['sentence']: np.reshape(x,[-1,15]), model.placeholders['cls']:np.reshape(y,[-1,5])})\n",
    "            train_accuracy=train_accuracy+train_acc['accuracy']\n",
    "            train_cost=train_cost+train_acc['cost']\n",
    "\n",
    "        print(f\"Training: Epoch: {i+1}, Cost: {train_cost/len(batched_train)}, Acc: {train_accuracy/len(batched_train)}\")\n",
    "        train_accuracy=0\n",
    "        train_cost=0\n",
    "        for batch in batched_test:\n",
    "            x, y=get_batch(batch)\n",
    "            if len(x)!=32:\n",
    "                print(len(x))\n",
    "                continue\n",
    "            acc=sess.run(model.acc,feed_dict={model.placeholders['sentence']: np.reshape(x,[-1,15]), model.placeholders['cls']:np.reshape(y,[-1,5])})\n",
    "            accuracy=accuracy+acc['accuracy']\n",
    "            cost=cost+acc['cost']\n",
    "\n",
    "        print(f\"Testing:  Epoch: {i+1}, Cost: {cost/len(batched_test)}, Acc: {accuracy/len(batched_test)}\")\n",
    "        accuracy=0\n",
    "        cost=0\n",
    "        f1=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the accuracy increasing by fine-tuning the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch: 1, Cost: 0.8755560705448662, Acc: 0.6506883479648843\n",
      "Testing:  Epoch: 1, Cost: 0.7434038513194258, Acc: 0.7091653354632588\n",
      "Training: Epoch: 2, Cost: 0.4329615601079043, Acc: 0.8397845171588189\n",
      "Testing:  Epoch: 2, Cost: 0.7860060959768752, Acc: 0.7170527156549521\n",
      "Training: Epoch: 3, Cost: 0.2055264384536324, Acc: 0.9283719074221868\n",
      "Testing:  Epoch: 3, Cost: 1.0693155011049094, Acc: 0.6981829073482428\n",
      "Training: Epoch: 4, Cost: 0.11644200213743687, Acc: 0.9612679569034318\n",
      "Testing:  Epoch: 4, Cost: 1.3170173526191102, Acc: 0.6962859424920128\n",
      "Training: Epoch: 5, Cost: 0.07428865747861504, Acc: 0.9754090183559457\n",
      "Testing:  Epoch: 5, Cost: 1.734269525201176, Acc: 0.6963857827476039\n"
     ]
    }
   ],
   "source": [
    "accuracy=0\n",
    "cost=0\n",
    "f1=0\n",
    "train_accuracy=0\n",
    "train_cost=0\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model=LSTM(train=True)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(5):\n",
    "        for batch in batched_train:\n",
    "            x, y=get_batch(batch)\n",
    "            \n",
    "            if len(x)!=32:\n",
    "                print(len(x))\n",
    "                continue\n",
    "\n",
    "            sess.run(model.train,feed_dict={model.placeholders['sentence']: np.reshape(x,[-1,15]), model.placeholders['cls']:np.reshape(y,[-1,5])})\n",
    "            train_acc=sess.run(model.acc,feed_dict={model.placeholders['sentence']: np.reshape(x,[-1,15]), model.placeholders['cls']:np.reshape(y,[-1,5])})\n",
    "            train_accuracy=train_accuracy+train_acc['accuracy']\n",
    "            train_cost=train_cost+train_acc['cost']\n",
    "\n",
    "        print(f\"Training: Epoch: {i+1}, Cost: {train_cost/len(batched_train)}, Acc: {train_accuracy/len(batched_train)}\")\n",
    "        train_accuracy=0\n",
    "        train_cost=0\n",
    "        for batch in batched_test:\n",
    "            x, y=get_batch(batch)\n",
    "            if len(x)!=32:\n",
    "                print(len(x))\n",
    "                continue\n",
    "            acc=sess.run(model.acc,feed_dict={model.placeholders['sentence']: np.reshape(x,[-1,15]), model.placeholders['cls']:np.reshape(y,[-1,5])})\n",
    "            accuracy=accuracy+acc['accuracy']\n",
    "            cost=cost+acc['cost']\n",
    "\n",
    "        print(f\"Testing:  Epoch: {i+1}, Cost: {cost/len(batched_test)}, Acc: {accuracy/len(batched_test)}\")\n",
    "        accuracy=0\n",
    "        cost=0\n",
    "        f1=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
